{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QXQlkOOPvYm8"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U langchain langchain-google-genai langchain-community chromadb pymupdf sentence-transformers google-ai-generativelanguage==0.6.15\n",
        "\n",
        "\n",
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "\n",
        "if \"GOOGLE_API_KEY\" not in os.environ:\n",
        "    os.environ[\"GOOGLE_API_KEY\"] = getpass(\"Enter your Google API Key: \")\n",
        "    print(\"Google API Key set.\")\n",
        "else:\n",
        "    print(\"Google API Key already set.\")\n",
        "\n",
        "import fitz\n",
        "import re\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.embeddings import SentenceTransformerEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.chains import create_retrieval_chain\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "import asyncio\n",
        "from typing import List, Dict\n",
        "\n",
        "# --- 1. PDF Text Extraction  ---\n",
        "def extract_text_from_pdf(pdf_path: str) -> str:\n",
        "    \"\"\"\n",
        "    Extracts text from a PDF document using PyMuPDF's 'text' mode.\n",
        "\n",
        "    Args:\n",
        "        pdf_path (str): The path to the PDF file.\n",
        "\n",
        "    Returns:\n",
        "        str: The concatenated text from all pages of the PDF.\n",
        "    \"\"\"\n",
        "    text = \"\"\n",
        "    try:\n",
        "        doc = fitz.open(pdf_path)\n",
        "        for page_num in range(doc.page_count):\n",
        "            page = doc.load_page(page_num)\n",
        "            text += page.get_text(\"text\")\n",
        "        doc.close()\n",
        "        print(f\"Successfully extracted text from {pdf_path} using 'text' mode.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting text from {pdf_path}: {e}\")\n",
        "    return text\n",
        "\n",
        "# --- 2. Text Pre-processing & Cleaning  ---\n",
        "def clean_text(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Performs robust text cleaning for PDF extracted content.\n",
        "    - Replaces common PDF artifacts (hyphenated words).\n",
        "    - Removes non-standard characters, replacing them with spaces.\n",
        "    - Normalizes all whitespace (spaces, tabs, newlines, unicode spaces) to single spaces.\n",
        "    - Consolidates multiple newlines into paragraph breaks.\n",
        "    - Strips leading/trailing whitespace.\n",
        "\n",
        "    Args:\n",
        "        text (str): The raw text extracted from the PDF.\n",
        "\n",
        "    Returns:\n",
        "        str: The cleaned text.\n",
        "    \"\"\"\n",
        "    # 1. Replace common PDF artifacts like hyphenated words across lines (e.g., \"word-\\nword\" -> \"wordword\")\n",
        "    text = re.sub(r'(\\w+)-\\s*\\n\\s*(\\w+)', r'\\1\\2', text)\n",
        "\n",
        "    # 2. Remove form feed characters\n",
        "    text = text.replace('\\x0c', '')\n",
        "\n",
        "    # 3. Normalize all types of whitespace to a single space.\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "    # 4. Remove any characters that are NOT standard English/Bengali letters, digits, or common punctuation.\n",
        "\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\u0980-\\u09FF.,!?;:()\\[\\]{}\\'\"\\-_ ]+', ' ', text)\n",
        "\n",
        "    # 5. Consolidate multiple spaces that might have resulted from step 4\n",
        "    text = re.sub(r' +', ' ', text)\n",
        "\n",
        "    # 6. Strip leading/trailing whitespace from the entire text\n",
        "    text = text.strip()\n",
        "\n",
        "    # 7. Re-introduce paragraph breaks by replacing single newlines with spaces and then double newlines\n",
        "    text = re.sub(r'(\\n\\s*){2,}', '\\n\\n', text)\n",
        "\n",
        "    print(\"Text cleaning complete.\")\n",
        "    return text.strip()\n",
        "\n",
        "# --- 3. Document Chunking & Vectorization with ChromaDB ---\n",
        "def create_and_vectorize_knowledge_base(text: str, db_path=\"chroma_db_hsc_bangla\") -> Chroma:\n",
        "    \"\"\"\n",
        "    Chunks text, creates embeddings, and stores them in a ChromaDB vector store.\n",
        "\n",
        "    Args:\n",
        "        text (str): The cleaned text corpus.\n",
        "        db_path (str): The path to store the ChromaDB persistent collection.\n",
        "\n",
        "    Returns:\n",
        "        Chroma: The initialized ChromaDB vector store.\n",
        "    \"\"\"\n",
        "    # Document Chunking\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=700,\n",
        "        chunk_overlap=100,\n",
        "        length_function=len,\n",
        "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
        "    )\n",
        "    chunks = text_splitter.split_text(text)\n",
        "    print(f\"Created {len(chunks)} chunks.\")\n",
        "\n",
        "    # Embedding Model\n",
        "    print(\"Loading embedding model: paraphrase-multilingual-MiniLM-L12-v2...\")\n",
        "    embeddings_model = SentenceTransformerEmbeddings(model_name=\"paraphrase-multilingual-MiniLM-L12-v2\")\n",
        "    print(\"Embedding model loaded.\")\n",
        "\n",
        "    # Vectorize and store in ChromaDB\n",
        "    os.makedirs(db_path, exist_ok=True)\n",
        "\n",
        "    print(f\"Creating/Loading ChromaDB at {db_path}...\")\n",
        "    vectorstore = Chroma.from_texts(\n",
        "        texts=chunks,\n",
        "        embedding=embeddings_model,\n",
        "        persist_directory=db_path\n",
        "    )\n",
        "    vectorstore.persist()\n",
        "    print(\"ChromaDB vector store created/loaded and persisted.\")\n",
        "    return vectorstore\n",
        "\n",
        "# --- 4. RAG Chain Setup with Google Gemini ---\n",
        "def setup_rag_chain(vectorstore: Chroma):\n",
        "    \"\"\"\n",
        "    Sets up the RAG chain using LangChain, Google Gemini, and ChromaDB.\n",
        "\n",
        "    Args:\n",
        "        vectorstore (Chroma): The initialized ChromaDB vector store.\n",
        "\n",
        "    Returns:\n",
        "        RetrievalQA: The RAG chain.\n",
        "    \"\"\"\n",
        "    # Initialize the LLM (Google Gemini Pro)\n",
        "    llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", temperature=0.2)\n",
        "\n",
        "    # Define the prompt for the LLM\n",
        "\n",
        "    # Contextualize the question based on chat history (for short-term memory)\n",
        "    contextualize_q_system_prompt = (\n",
        "        \"Given a chat history and the latest user question \"\n",
        "        \"which might reference context in the chat history, \"\n",
        "        \"formulate a standalone question which can be understood without \"\n",
        "        \"the chat history. Do NOT answer the question, just reformulate it \"\n",
        "        \"if necessary and otherwise return it as is.\"\n",
        "    )\n",
        "    contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
        "        [\n",
        "            (\"system\", contextualize_q_system_prompt),\n",
        "            MessagesPlaceholder(\"chat_history\"),\n",
        "            (\"human\", \"{input}\"),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    # Combined with RAG - main QA prompt\n",
        "    qa_system_prompt = (\n",
        "        \"You are an assistant for question-answering tasks. \"\n",
        "        \"Use the following retrieved context to answer the question. \"\n",
        "        \"If you don't know the answer, just say that you don't know. \"\n",
        "        \"Do not make up an answer. \"\n",
        "        \"Keep the answer concise and to the point, usually one or two sentences. \"\n",
        "        \"Answer in the same language as the question.\\n\\n\"\n",
        "        \"{context}\"\n",
        "    )\n",
        "    qa_prompt = ChatPromptTemplate.from_messages(\n",
        "        [\n",
        "            (\"system\", qa_system_prompt),\n",
        "            MessagesPlaceholder(\"chat_history\"),\n",
        "            (\"human\", \"{input}\"),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    # Create a chain to combine documents and answer the question\n",
        "    document_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
        "\n",
        "    # Create the retrieval chain\n",
        "    retrieval_chain = create_retrieval_chain(\n",
        "        vectorstore.as_retriever(), # Use the vectorstore as a retriever\n",
        "        document_chain\n",
        "    )\n",
        "\n",
        "    print(\"RAG chain setup complete.\")\n",
        "    return retrieval_chain\n",
        "\n",
        "# --- 5. Function to get RAG response with chat history ---\n",
        "async def get_rag_response_with_history(rag_chain, question: str, chat_history: List[Dict]):\n",
        "    \"\"\"\n",
        "    Gets a response from the RAG chain, managing chat history.\n",
        "    Returns the generated answer and the retrieved documents.\n",
        "    \"\"\"\n",
        "    # Convert chat_history from dict to LangChain's message objects\n",
        "    lc_chat_history = []\n",
        "    for message in chat_history:\n",
        "        if message[\"role\"] == \"user\":\n",
        "            lc_chat_history.append(HumanMessage(content=message[\"content\"]))\n",
        "        elif message[\"role\"] == \"ai\":\n",
        "            lc_chat_history.append(AIMessage(content=message[\"content\"]))\n",
        "\n",
        "    response = await rag_chain.ainvoke({\"input\": question, \"chat_history\": lc_chat_history})\n",
        "    return response[\"answer\"], response[\"context\"]\n",
        "# --- Main Execution Block ---\n",
        "pdf_file_path = \"/content/HSC26-Bangla1st-Paper.pdf\"\n",
        "chroma_db_path = \"./chroma_db_hsc_bangla\"\n",
        "\n",
        "# Check if the PDF exists\n",
        "if not os.path.exists(pdf_file_path):\n",
        "    print(f\"Error: PDF file not found at '{pdf_file_path}'\")\n",
        "    print(\"Please upload 'HSC26_Bangla_1st_paper.pdf' to your Colab environment.\")\n",
        "else:\n",
        "    print(f\"PDF file '{pdf_file_path}' found.\")\n",
        "\n",
        "# Initialize RAG components\n",
        "rag_chain = None\n",
        "vector_db = None\n",
        "\n",
        "# Check if the database already exists and load it.\n",
        "if os.path.exists(chroma_db_path) and os.path.isdir(chroma_db_path) and len(os.listdir(chroma_db_path)) > 0:\n",
        "    print(f\"Loading existing ChromaDB from {chroma_db_path}...\")\n",
        "    embeddings_model = SentenceTransformerEmbeddings(model_name=\"paraphrase-multilingual-MiniLM-L12-v2\")\n",
        "    vector_db = Chroma(persist_directory=chroma_db_path, embedding_function=embeddings_model)\n",
        "    print(\"ChromaDB loaded.\")\n",
        "else:\n",
        "    print(\"ChromaDB not found or empty. Creating new knowledge base...\")\n",
        "    if os.path.exists(pdf_file_path):\n",
        "        corpus_text = extract_text_from_pdf(pdf_file_path)\n",
        "        cleaned_corpus_text = clean_text(corpus_text)\n",
        "        if cleaned_corpus_text.strip():\n",
        "            vector_db = create_and_vectorize_knowledge_base(cleaned_corpus_text, db_path=chroma_db_path)\n",
        "            print(\"Knowledge base created and vectorized.\")\n",
        "        else:\n",
        "            print(\"Extracted and cleaned text is empty. Cannot create knowledge base.\")\n",
        "    else:\n",
        "        print(\"Cannot create knowledge base: PDF file not found.\")\n",
        "\n",
        "if vector_db:\n",
        "    rag_chain = setup_rag_chain(vector_db)\n",
        "    print(\"RAG system initialized successfully. You can now ask questions!\")\n",
        "else:\n",
        "    print(\"RAG system could not be initialized. Please check the PDF file and API key.\")\n",
        "\n",
        "# --- Interactive Query Loop ---\n",
        "if rag_chain:\n",
        "    chat_history = []\n",
        "    print(\"\\n--- Start Chatting ---\")\n",
        "    print(\"Type 'exit' or 'quit' to end the conversation.\")\n",
        "\n",
        "    while True:\n",
        "        user_question = input(\"\\nYour Question (English/Bengali): \")\n",
        "        if user_question.lower() in [\"exit\", \"quit\"]:\n",
        "            print(\"Exiting chat. Goodbye!\")\n",
        "            break\n",
        "\n",
        "        try:\n",
        "            # Get response from RAG chain\n",
        "            answer, retrieved_docs = await get_rag_response_with_history(rag_chain, user_question, chat_history)\n",
        "\n",
        "            print(f\"\\nAI Answer: {answer}\")\n",
        "\n",
        "\n",
        "\n",
        "            # Update chat history (short-term memory)\n",
        "            chat_history.append({\"role\": \"user\", \"content\": user_question})\n",
        "            chat_history.append({\"role\": \"ai\", \"content\": answer})\n",
        "            # Keep only the last few turns (e.g., 4 messages: 2 user, 2 AI)\n",
        "            chat_history = chat_history[-4:]\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred during query: {e}\")\n",
        "            print(\"Please ensure your Google API Key is valid and the PDF content is suitable.\")\n"
      ]
    }
  ]
}